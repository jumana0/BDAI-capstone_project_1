{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "import plotly.express as px\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from rich import print\n",
    "from rich.progress import track\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import Lasso, Ridge, ElasticNet,LassoCV,RidgeCV\n",
    "from sklearn.model_selection import cross_val_score, train_test_split,KFold, GridSearchCV\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JawwyTV_df = pd.read_csv(\"Final_Dataset.txt\",sep=\",\")\n",
    "JawwyTV_df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JawwyTV_df.drop(columns=\"Unnamed: 0\",inplace=True) #drop column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JawwyTV_df.isnull().sum()#Find the null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_program = pd.DataFrame(JawwyTV_df[['original_name'].drop_duplicates(), columns=[\"original_name\"])#Take Only the Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_program['url']=''#Add a column for url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_program['dec']=''#Add a column for dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_program.reset_index(inplace=True)#Rerange the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url(search_term):#scrapping links from imdb\n",
    "    \"\"\"Generate url with search term\"\"\"\n",
    "    \n",
    "    template = \"https://www.imdb.com/find?q={}&ref_=nv_sr_sm\"#the link for search \n",
    "    search_term = search_term.replace(\" \", \"+\")#any space are there in names\n",
    "    \n",
    "    \n",
    "    # Add search term to url\n",
    "    url = template.format(search_term)\n",
    "\n",
    "    \n",
    "    return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_record(item):\n",
    "    \"\"\"Extract and return data from single record\"\"\"\n",
    "\n",
    "    atag = item.a\n",
    "    new_url = \"https://www.imdb.com\"+atag.get('href')#Taking the link for each name\n",
    "\n",
    "    res = new_url\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(search_term): # this find the link for each Name\n",
    "    \n",
    "    chromedriver = \"C:\\Program Files\\Google\\Chrome\\Application\\chromedriver\"\n",
    "    os.environ[\"webdriver.chrome.driver\"] = chromedriver\n",
    "    DRIVER = webdriver.Chrome(chromedriver)\n",
    "    RES = None\n",
    "    URL = get_url(search_term)\n",
    "    \n",
    "    for page in range(1):\n",
    "        DRIVER.get(URL.format(page))\n",
    "        SOUP = BeautifulSoup(DRIVER.page_source, 'html.parser')\n",
    "        RESULTS = SOUP.find_all('div', {'class': 'ipc-metadata-list-summary-item__tc'})#Find the div for taking the main link for each name\n",
    "        for item in RESULTS:\n",
    "            RES = extract_record(item)\n",
    "            print(RES)\n",
    "            if RES:\n",
    "                break\n",
    "    \n",
    "    DRIVER.close()\n",
    "    \n",
    "    \n",
    "    return RES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(URL):#this find desc for each link\n",
    "    chromedriver = \"C:\\Program Files\\Google\\Chrome\\Application\\chromedriver\"\n",
    "    os.environ[\"webdriver.chrome.driver\"] = chromedriver\n",
    "    DRIVER = webdriver.Chrome(chromedriver)\n",
    "\n",
    "    dec = None\n",
    "    for url in track(URL, description='[green]Stealing in progress...'):\n",
    "        DRIVER.get(url)\n",
    "        SOUP = BeautifulSoup(DRIVER.page_source, 'html.parser')\n",
    "        try:\n",
    "                if SOUP.findAll('div',{ 'class':\"sc-16ede01-8 hXeKyz sc-7643a8e3-11 efPxUc\"})!=None:#If the div was not there in website will go to find the other one\n",
    "                    dec = SOUP.find('span',{'class':'sc-16ede01-0 fMPjMP'}).text\n",
    "                    \n",
    "                elif  SOUP.findAll('div',{ 'class':\"sc-16ede01-7 hrgVKw\"})!=None:\n",
    "                        dec = SOUP.find('span',{'class':'sc-16ede01-0 fMPjMP'}).text\n",
    "                else:\n",
    "                       pass\n",
    "                \n",
    "        except:\n",
    "             dec = None\n",
    "    return dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "index = 0 #This Function for find the imdb link and find the text description\n",
    "for i in all_program['original_name']:\n",
    "    \n",
    "         for t in range(4):# we put for_loop beacuse some times doesnt extract the text then it will return to find it again \n",
    "              all_program.replace(np.nan,'',inplace=True)#To Replace any Nan value to empty string \n",
    "              if all_program.loc[index,'url'] == '':\n",
    "                   url = main(i)\n",
    "                   all_program.loc[index,'url'] = url\n",
    "                   if url != None:\n",
    "                        all_program.loc[index,'dec'] = get_data([url])\n",
    "\n",
    "              elif all_program.loc[index,'dec'] == ''and all_program.loc[index,'url']!=None :\n",
    "                        all_program.loc[index,'dec'] = get_data([all_program.loc[index,'url']])\n",
    "                    \n",
    "\n",
    "              else: break\n",
    "\n",
    "         index = index +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_program.to_csv('Get_Desc.csv')#Save DataFrame to CSV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "b2dbf1203128af0b6968be9bada9935508394635760a08dff4f40d298095f607"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
